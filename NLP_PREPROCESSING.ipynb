{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c00265c",
      "metadata": {
        "id": "4c00265c"
      },
      "outputs": [],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP. Identifying stopwords helps in focusing on meaningful words. Dependency parsing and POS tagging provide insights into the structure of sentences. Named Entity Recognition (NER) identifies entities like persons, organizations, and locations. Chunking groups words into meaningful chunks. HTML tags, such as <b> and <i>, can be removed for plain text. Emojis like ðŸ˜ƒ add a layer of emotion to text. Removing special characters is crucial for text processing. Spelling corrections can improve the overall quality of the text. Understanding the sentiment and emotion behind text is important for various applications.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc041d1b",
      "metadata": {
        "id": "fc041d1b",
        "outputId": "244d819d-ed57-42bc-c3e4-4368b69f1b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5efa23",
      "metadata": {
        "id": "fe5efa23",
        "outputId": "7eb6e6b1-633a-4cae-8c85-550959b91651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: spacy in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b37fb87",
      "metadata": {
        "id": "0b37fb87",
        "outputId": "90c16ee7-c687-4906-d44d-f3fd91b92175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c5fde3",
      "metadata": {
        "id": "e7c5fde3",
        "outputId": "7f110b80-6a95-4178-b790-461bfc5dca71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting emoji\n",
            "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/03/40/91d0c9fe5a0b494c0fdbcacda4d203aea39f8293e69c70129389308ca928/emoji-2.9.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "   ---------------------------------------- 0.0/397.5 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/397.5 kB ? eta -:--:--\n",
            "   -------- ------------------------------- 81.9/397.5 kB 2.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 397.5/397.5 kB 6.2 MB/s eta 0:00:00\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a7fe52",
      "metadata": {
        "id": "42a7fe52",
        "outputId": "94c27a23-234d-4811-d525-12826dfbc813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
            "      --------------------------------------- 10.2/636.8 kB ? eta -:--:--\n",
            "     ----- ------------------------------- 92.2/636.8 kB 871.5 kB/s eta 0:00:01\n",
            "     -------------------------------------- 636.8/636.8 kB 4.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
            "Installing collected packages: textblob\n",
            "Successfully installed textblob-0.17.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a35783c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a35783c",
        "outputId": "f6dc02b3-18e4-4e07-cd6b-e1f8ebca8537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebe67c6",
      "metadata": {
        "id": "aebe67c6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "# from bs4 import BeautifulSoup\n",
        "# import emoji\n",
        "from textblob import TextBlob\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7085d7",
      "metadata": {
        "id": "fc7085d7"
      },
      "source": [
        "# 1. Word Tokenization:\n",
        "Task: Break the text into individual words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bd99a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78bd99a7",
        "outputId": "622c9946-3ec1-44ea-c6ec-7be7a318fa4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.', 'Identifying', 'stopwords', 'helps', 'in', 'focusing', 'on', 'meaningful', 'words', '.', 'Dependency', 'parsing', 'and', 'POS', 'tagging', 'provide', 'insights', 'into', 'the', 'structure', 'of', 'sentences', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'persons', ',', 'organizations', ',', 'and', 'locations', '.', 'Chunking', 'groups', 'words', 'into', 'meaningful', 'chunks', '.', 'HTML', 'tags', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'removed', 'for', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emotion', 'to', 'text', '.', 'Removing', 'special', 'characters', 'is', 'crucial', 'for', 'text', 'processing', '.', 'Spelling', 'corrections', 'can', 'improve', 'the', 'overall', 'quality', 'of', 'the', 'text', '.', 'Understanding', 'the', 'sentiment', 'and', 'emotion', 'behind', 'text', 'is', 'important', 'for', 'various', 'applications', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d7569c",
      "metadata": {
        "id": "25d7569c"
      },
      "source": [
        "# 2. Stemming:\n",
        "Task: Reduce words to their root/base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697a9b1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "697a9b1d",
        "outputId": "4e01ee4d-ae53-4734-f2e3-a11726a95b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'fascin', 'field', '.', 'it', 'involv', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'languag', '.', 'token', ',', 'stem', ',', 'and', 'lemmat', 'are', 'essenti', 'step', 'in', 'nlp', '.', 'identifi', 'stopword', 'help', 'in', 'focus', 'on', 'meaning', 'word', '.', 'depend', 'pars', 'and', 'po', 'tag', 'provid', 'insight', 'into', 'the', 'structur', 'of', 'sentenc', '.', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'identifi', 'entiti', 'like', 'person', ',', 'organ', ',', 'and', 'locat', '.', 'chunk', 'group', 'word', 'into', 'meaning', 'chunk', '.', 'html', 'tag', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'remov', 'for', 'plain', 'text', '.', 'emoji', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emot', 'to', 'text', '.', 'remov', 'special', 'charact', 'is', 'crucial', 'for', 'text', 'process', '.', 'spell', 'correct', 'can', 'improv', 'the', 'overal', 'qualiti', 'of', 'the', 'text', '.', 'understand', 'the', 'sentiment', 'and', 'emot', 'behind', 'text', 'is', 'import', 'for', 'variou', 'applic', '.']\n"
          ]
        }
      ],
      "source": [
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in tokens]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1224d709",
      "metadata": {
        "id": "1224d709"
      },
      "source": [
        "# 3.Stemming:\n",
        "Reduces words to their root form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2fb042",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2fb042",
        "outputId": "cf265555-5fd2-45b5-86a3-f7c5dcedefd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'fascin', 'field', '.', 'it', 'involv', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'languag', '.', 'token', ',', 'stem', ',', 'and', 'lemmat', 'are', 'essenti', 'step', 'in', 'nlp', '.', 'identifi', 'stopword', 'help', 'in', 'focus', 'on', 'meaning', 'word', '.', 'depend', 'pars', 'and', 'po', 'tag', 'provid', 'insight', 'into', 'the', 'structur', 'of', 'sentenc', '.', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'identifi', 'entiti', 'like', 'person', ',', 'organ', ',', 'and', 'locat', '.', 'chunk', 'group', 'word', 'into', 'meaning', 'chunk', '.', 'html', 'tag', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'remov', 'for', 'plain', 'text', '.', 'emoji', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emot', 'to', 'text', '.', 'remov', 'special', 'charact', 'is', 'crucial', 'for', 'text', 'process', '.', 'spell', 'correct', 'can', 'improv', 'the', 'overal', 'qualiti', 'of', 'the', 'text', '.', 'understand', 'the', 'sentiment', 'and', 'emot', 'behind', 'text', 'is', 'import', 'for', 'variou', 'applic', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "Stemmed_words=[ps.stem(word) for word in tokens]\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245f5288",
      "metadata": {
        "id": "245f5288"
      },
      "source": [
        "# 4. Lemmatization\n",
        "Reduce words to their base form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2b461a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2b461a",
        "outputId": "c777b1f6-3782-44c5-94ff-77178fd6ca11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'step', 'in', 'NLP', '.', 'Identifying', 'stopwords', 'help', 'in', 'focusing', 'on', 'meaningful', 'word', '.', 'Dependency', 'parsing', 'and', 'POS', 'tagging', 'provide', 'insight', 'into', 'the', 'structure', 'of', 'sentence', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entity', 'like', 'person', ',', 'organization', ',', 'and', 'location', '.', 'Chunking', 'group', 'word', 'into', 'meaningful', 'chunk', '.', 'HTML', 'tag', ',', 'such', 'a', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'removed', 'for', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emotion', 'to', 'text', '.', 'Removing', 'special', 'character', 'is', 'crucial', 'for', 'text', 'processing', '.', 'Spelling', 'correction', 'can', 'improve', 'the', 'overall', 'quality', 'of', 'the', 'text', '.', 'Understanding', 'the', 'sentiment', 'and', 'emotion', 'behind', 'text', 'is', 'important', 'for', 'various', 'application', '.']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatized words:\",lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b10b8e7",
      "metadata": {
        "id": "0b10b8e7"
      },
      "source": [
        "# 5. Identifying Stopword\n",
        "are common words that are often removed from text for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87586082",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87586082",
        "outputId": "7631e670-71bc-4202-fdbe-f94c8837fe08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (without stopwords): ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', '.', 'involves', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'lemmatization', 'essential', 'steps', 'NLP', '.', 'Identifying', 'stopwords', 'helps', 'focusing', 'meaningful', 'words', '.', 'Dependency', 'parsing', 'POS', 'tagging', 'provide', 'insights', 'structure', 'sentences', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'persons', ',', 'organizations', ',', 'locations', '.', 'Chunking', 'groups', 'words', 'meaningful', 'chunks', '.', 'HTML', 'tags', ',', '<', 'b', '>', '<', '>', ',', 'removed', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'layer', 'emotion', 'text', '.', 'Removing', 'special', 'characters', 'crucial', 'text', 'processing', '.', 'Spelling', 'corrections', 'improve', 'overall', 'quality', 'text', '.', 'Understanding', 'sentiment', 'emotion', 'behind', 'text', 'important', 'various', 'applications', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered Tokens (without stopwords):\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc113554",
      "metadata": {
        "id": "fc113554"
      },
      "source": [
        "# 6.  Dependency Parsing\n",
        "Dependency parsing analyzes the grammatical structure of sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a488389",
      "metadata": {
        "id": "8a488389",
        "outputId": "f0a42631-060a-4ef7-b503-d73cc2e8fdff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: spacy in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f17efc",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32f17efc",
        "outputId": "a601b4a0-6443-4ae9-a8d1-4100cb1ba547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency compound\n",
            "parsing nsubj\n",
            "is ROOT\n",
            "an det\n",
            "interesting amod\n",
            "NLP compound\n",
            "task attr\n",
            ". punct\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Dependency parsing is an interesting NLP task.\")\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a37cd50",
      "metadata": {
        "id": "7a37cd50"
      },
      "source": [
        "# 7. Part-of-Speech (POS) Tagging:\n",
        "POS tagging labels each word with its grammatical category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01061a51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01061a51",
        "outputId": "ff96ba7d-ae32-4441-b1a2-5aacf96510df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'fascinating',\n",
              " 'field',\n",
              " '.',\n",
              " 'It',\n",
              " 'involves',\n",
              " 'the',\n",
              " 'interaction',\n",
              " 'between',\n",
              " 'computers',\n",
              " 'and',\n",
              " 'humans',\n",
              " 'using',\n",
              " 'natural',\n",
              " 'language',\n",
              " '.',\n",
              " 'Tokenization',\n",
              " ',',\n",
              " 'stemming',\n",
              " ',',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " 'are',\n",
              " 'essential',\n",
              " 'steps',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.',\n",
              " 'Identifying',\n",
              " 'stopwords',\n",
              " 'helps',\n",
              " 'in',\n",
              " 'focusing',\n",
              " 'on',\n",
              " 'meaningful',\n",
              " 'words',\n",
              " '.',\n",
              " 'Dependency',\n",
              " 'parsing',\n",
              " 'and',\n",
              " 'POS',\n",
              " 'tagging',\n",
              " 'provide',\n",
              " 'insights',\n",
              " 'into',\n",
              " 'the',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'sentences',\n",
              " '.',\n",
              " 'Named',\n",
              " 'Entity',\n",
              " 'Recognition',\n",
              " '(',\n",
              " 'NER',\n",
              " ')',\n",
              " 'identifies',\n",
              " 'entities',\n",
              " 'like',\n",
              " 'persons',\n",
              " ',',\n",
              " 'organizations',\n",
              " ',',\n",
              " 'and',\n",
              " 'locations',\n",
              " '.',\n",
              " 'Chunking',\n",
              " 'groups',\n",
              " 'words',\n",
              " 'into',\n",
              " 'meaningful',\n",
              " 'chunks',\n",
              " '.',\n",
              " 'HTML',\n",
              " 'tags',\n",
              " ',',\n",
              " 'such',\n",
              " 'as',\n",
              " '<',\n",
              " 'b',\n",
              " '>',\n",
              " 'and',\n",
              " '<',\n",
              " 'i',\n",
              " '>',\n",
              " ',',\n",
              " 'can',\n",
              " 'be',\n",
              " 'removed',\n",
              " 'for',\n",
              " 'plain',\n",
              " 'text',\n",
              " '.',\n",
              " 'Emojis',\n",
              " 'like',\n",
              " 'ðŸ˜ƒ',\n",
              " 'add',\n",
              " 'a',\n",
              " 'layer',\n",
              " 'of',\n",
              " 'emotion',\n",
              " 'to',\n",
              " 'text',\n",
              " '.',\n",
              " 'Removing',\n",
              " 'special',\n",
              " 'characters',\n",
              " 'is',\n",
              " 'crucial',\n",
              " 'for',\n",
              " 'text',\n",
              " 'processing',\n",
              " '.',\n",
              " 'Spelling',\n",
              " 'corrections',\n",
              " 'can',\n",
              " 'improve',\n",
              " 'the',\n",
              " 'overall',\n",
              " 'quality',\n",
              " 'of',\n",
              " 'the',\n",
              " 'text',\n",
              " '.',\n",
              " 'Understanding',\n",
              " 'the',\n",
              " 'sentiment',\n",
              " 'and',\n",
              " 'emotion',\n",
              " 'behind',\n",
              " 'text',\n",
              " 'is',\n",
              " 'important',\n",
              " 'for',\n",
              " 'various',\n",
              " 'applications',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219a9a91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "219a9a91",
        "outputId": "44ed3f01-54b2-4986-b523-6d4bd6af9ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bf88dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64bf88dc",
        "outputId": "0cfea2f7-811d-4909-88ac-9cc4f2f20769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
        "token_list = word_tokenize(text)\n",
        "print(\"Word Tokens:\", token_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d3a0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4d3a0c8",
        "outputId": "c5d6d899-dd67-4249-b5d5-05671cbe987c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
        "token_list = word_tokenize(text)\n",
        "print(\"Word Tokens:\", token_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceceec2f",
      "metadata": {
        "id": "ceceec2f"
      },
      "source": [
        "# 8. Named Entity Recognition (NER):\n",
        "     NER identifies entities like persons, organizations, and locations in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e536873",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e536873",
        "outputId": "c5ea17bc-06f2-46a6-ac79-6b3c244c175c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nlp) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (10.0.1)\n",
            "Collecting dill (from nlp)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nlp) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from nlp) (3.13.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from nlp) (3.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: dill, nlp\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 449, in run\n",
            "    installed = install_given_reqs(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/__init__.py\", line 72, in install_given_reqs\n",
            "    requirement.install(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 800, in install\n",
            "    install_wheel(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 731, in install_wheel\n",
            "    _install_wheel(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/install/wheel.py\", line 617, in _install_wheel\n",
            "    success = compileall.compile_file(path, force=True, quiet=True)\n",
            "  File \"/usr/lib/python3.10/compileall.py\", line 240, in compile_file\n",
            "    ok = py_compile.compile(fullname, cfile, dfile, True,\n",
            "  File \"/usr/lib/python3.10/py_compile.py\", line 172, in compile\n",
            "    importlib._bootstrap_external._write_atomic(cfile, bytecode, mode)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 198, in _write_atomic\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 172, in emit\n",
            "    style = Style(color=\"red\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/style.py\", line 149, in __init__\n",
            "    self._color = None if color is None else _make_color(color)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/style.py\", line 147, in _make_color\n",
            "    return color if isinstance(color, Color) else Color.parse(color)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f09873",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96f09873",
        "outputId": "9c9a014b-326c-4585-a6bb-de8a103e7d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('NLP', 'ORG'), ('NLP', 'ORG')]\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Named Entities:\", entities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d90c68d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d90c68d",
        "outputId": "8b268ba6-d585-48e7-c675-ad663dafa680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('NLP', 'ORG'), ('NLP', 'ORG')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Your text data\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Named Entities:\", entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e596aee0",
      "metadata": {
        "id": "e596aee0"
      },
      "source": [
        "# 9.  Chunking:\n",
        "    Chunking groups words into meaningful chunks based on grammatical patterns. This example defines a simple grammar for noun phrases (NP)."
      ]
    },
    {
      "source": [
        "pos_tags = nltk.pos_tag(token_list)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnjeXwBEpiXQ"
      },
      "id": "SnjeXwBEpiXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "tree = chunk_parser.parse(pos_tags)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZsAj62KxpkHs"
      },
      "id": "ZsAj62KxpkHs",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "pos_tags = nltk.pos_tag(token_list)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MIkN3pP2plAU"
      },
      "id": "MIkN3pP2plAU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "print(\"Chunks:\", tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_ar4O92pc2S",
        "outputId": "86acbf36-a383-4468-b34f-b4b9ea06fde6"
      },
      "id": "7_ar4O92pc2S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks: (S\n",
            "  Natural/JJ\n",
            "  Language/NNP\n",
            "  Processing/NNP\n",
            "  (/(\n",
            "  NLP/NNP\n",
            "  )/)\n",
            "  is/VBZ\n",
            "  (NP a/DT fascinating/JJ field/NN)\n",
            "  ./.\n",
            "  It/PRP\n",
            "  involves/VBZ\n",
            "  (NP the/DT interaction/NN)\n",
            "  between/IN\n",
            "  computers/NNS\n",
            "  and/CC\n",
            "  humans/NNS\n",
            "  using/VBG\n",
            "  (NP natural/JJ language/NN)\n",
            "  ./.\n",
            "  Tokenization/NNP\n",
            "  ,/,\n",
            "  (NP stemming/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP lemmatization/NN)\n",
            "  are/VBP\n",
            "  essential/JJ\n",
            "  steps/NNS\n",
            "  in/IN\n",
            "  NLP/NNP\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8bb9739",
      "metadata": {
        "id": "f8bb9739"
      },
      "source": [
        "# 10.HTML Tags::\n",
        "    BeautifulSoup is used to parse HTML tags and extract plain text.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba3e724",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ba3e724",
        "outputId": "e3e7fc01-9f04-4dad-acc3-76a4ee63d583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plain Text (without HTML tags): Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_text = text\n",
        "soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "plain_text = soup.get_text()\n",
        "print(\"Plain Text (without HTML tags):\", plain_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ff91c3",
      "metadata": {
        "id": "53ff91c3"
      },
      "source": [
        "# 11.Emoji:\n",
        "    This step extracts emojis from the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a87d1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a87d1b",
        "outputId": "f2d811a4-b99f-4245-f937-17d8f77606e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd533de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dd533de",
        "outputId": "bd2b6663-2c62-4fe1-9521-d7ae35a89355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emojis: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 'P', 'y', 't', 'h', 'o', 'n', '!', ' ']\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "text_with_emoji = \"I love Python! ðŸ˜ƒðŸ\"\n",
        "\n",
        "emoji_list = [c for c in text_with_emoji if c in emoji.demojize(text_with_emoji)]\n",
        "print(\"Emojis:\", emoji_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aaf64d5",
      "metadata": {
        "id": "0aaf64d5"
      },
      "source": [
        "# 12. Remove Special Characters:\n",
        "   Explanation: Regular expressions are used to remove special characters from the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c10408",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c10408",
        "outputId": "90f29afb-b041-4278-a339-17e1d42168da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "txt=text\n",
        "clean_txt=re.sub(r'[^a-zA-Z0-9\\s]', '',txt)\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713c9715",
      "metadata": {
        "id": "713c9715"
      },
      "source": [
        "\n",
        "# 13. Spelling Corrections:\n",
        "    Explanation: TextBlob is used for automatic spelling corrections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64034fa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64034fa1",
        "outputId": "cf75b025-2ca3-4055-e279-3caa301d12f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Text: Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, steaming, and lemmatization are essential steps in NLP.\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "txt=text\n",
        "blob=TextBlob(text)\n",
        "corrected_text=str(blob.correct())\n",
        "print(\"Corrected Text:\", corrected_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa762f2",
      "metadata": {
        "id": "baa762f2"
      },
      "source": [
        "# 14. Emotion Analysis:\n",
        "    TextBlob is used for sentiment analysis and emotion assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6456aef6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6456aef6",
        "outputId": "1fde12ce-3554-4b53-9c1c-edd82cce4190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Sentiment(polarity=0.22499999999999998, subjectivity=0.4875)\n",
            "Emotion: Sentiment(polarity=0.22499999999999998, subjectivity=0.4875, assessments=[(['natural'], 0.1, 0.4, None), (['fascinating'], 0.7, 0.8500000000000001, None), (['natural'], 0.1, 0.4, None), (['essential'], 0.0, 0.3, None)])\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "txt=text\n",
        "# text = \"I am so happy today!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Accessing sentiment and emotion\n",
        "sentiment = blob.sentiment\n",
        "emotion = blob.sentiment_assessments\n",
        "\n",
        "print(\"Sentiment:\", sentiment)\n",
        "print(\"Emotion:\", emotion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45dfea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b45dfea4",
        "outputId": "e26ab50c-1a6b-4c5d-8308-fcf817ec9a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: Sentiment(polarity=0.22499999999999998, subjectivity=0.4875)\n",
            "Emotion: Sentiment(polarity=0.22499999999999998, subjectivity=0.4875, assessments=[(['natural'], 0.1, 0.4, None), (['fascinating'], 0.7, 0.8500000000000001, None), (['natural'], 0.1, 0.4, None), (['essential'], 0.0, 0.3, None)])\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "txt=text\n",
        "# text = \"I am so happy today!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Accessing sentiment and emotion\n",
        "sentiment = blob.sentiment\n",
        "emotion = blob.sentiment_assessments\n",
        "\n",
        "print(\"Sentiment:\", sentiment)\n",
        "print(\"Emotion:\", emotion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd51786",
      "metadata": {
        "id": "3bd51786"
      },
      "source": [
        "# 15. Remove Numbers:\n",
        "Explanation: This step removes numeric digits from the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe37291",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fe37291",
        "outputId": "7d2c85e6-c1e5-4463-fd7d-aee6187fcefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without Numbers: Processing text with numbers like  and .\n"
          ]
        }
      ],
      "source": [
        "text_with_numbers = \"Processing text with numbers like 123 and 456.\"\n",
        "cleaned_text = re.sub(r'\\d+', '', text_with_numbers)\n",
        "print(\"Text without Numbers:\", cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136e7c44",
      "metadata": {
        "id": "136e7c44"
      },
      "source": [
        "# 16. Lowercasing:\n",
        "Explanation: Convert all text to lowercase. This helps in standardizing text for further analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e733983f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e733983f",
        "outputId": "0f02a568-c47b-462e-f7d6-9714bf0d69bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased Text: natural language processing (nlp) is a fascinating field. it involves the interaction between computers and humans using natural language. tokenization, stemming, and lemmatization are essential steps in nlp.\n"
          ]
        }
      ],
      "source": [
        "lowercased_text = text.lower()\n",
        "print(\"Lowercased Text:\", lowercased_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "505dd3c4",
      "metadata": {
        "id": "505dd3c4"
      },
      "source": [
        "# 17. Remove Punctuation:\n",
        "Explanation: Remove punctuation marks from the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf7b98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caf7b98c",
        "outputId": "62970ab2-21ed-4c1b-ff19-c445a892487e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without Punctuation: Text with some punctuations\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "text_with_punctuation = \"Text with! some@ punctuations#.\"\n",
        "cleaned_text = text_with_punctuation.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(\"Text without Punctuation:\", cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e0555b",
      "metadata": {
        "id": "e8e0555b"
      },
      "source": [
        "# 18. Tokenization (Sentence-level):\n",
        " Tokenize the text into sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e4a1d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e4a1d1",
        "outputId": "6687a662-572a-45f6-a6b7-c62c73621365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing (NLP) is a fascinating field.', 'It involves the interaction between computers and humans using natural language.', 'Tokenization, stemming, and lemmatization are essential steps in NLP.', 'Identifying stopwords helps in focusing on meaningful words.', 'Dependency parsing and POS tagging provide insights into the structure of sentences.', 'Named Entity Recognition (NER) identifies entities like persons, organizations, and locations.', 'Chunking groups words into meaningful chunks.', 'HTML tags, such as <b> and <i>, can be removed for plain text.', 'Emojis like ðŸ˜ƒ add a layer of emotion to text.', 'Removing special characters is crucial for text processing.', 'Spelling corrections can improve the overall quality of the text.', 'Understanding the sentiment and emotion behind text is important for various applications.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP. Identifying stopwords helps in focusing on meaningful words. Dependency parsing and POS tagging provide insights into the structure of sentences. Named Entity Recognition (NER) identifies entities like persons, organizations, and locations. Chunking groups words into meaningful chunks. HTML tags, such as <b> and <i>, can be removed for plain text. Emojis like ðŸ˜ƒ add a layer of emotion to text. Removing special characters is crucial for text processing. Spelling corrections can improve the overall quality of the text. Understanding the sentiment and emotion behind text is important for various applications.\"\n",
        "\n",
        "sentences=sent_tokenize(text)\n",
        "print(\"Sentences:\",sentences )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf910c9",
      "metadata": {
        "id": "5bf910c9"
      },
      "source": [
        "# 19. Bag of Words (BoW) Representation:\n",
        "Convert text into a matrix of token counts using the Bag of Words model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b35c49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b35c49d",
        "outputId": "3a130e33-b6c2-44a6-e9f6-edcb7f024d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of words(BOW) Representation:\n",
            "[[1 1 1 0 1 1]\n",
            " [2 0 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\"This is the first document.\", \"This document is the second document.\"]\n",
        "vect=CountVectorizer()\n",
        "x=vect.fit_transform(corpus)\n",
        "print(\"Bag of words(BOW) Representation:\")\n",
        "print(x.toarray())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}