{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c00265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP. Identifying stopwords helps in focusing on meaningful words. Dependency parsing and POS tagging provide insights into the structure of sentences. Named Entity Recognition (NER) identifies entities like persons, organizations, and locations. Chunking groups words into meaningful chunks. HTML tags, such as <b> and <i>, can be removed for plain text. Emojis like ðŸ˜ƒ add a layer of emotion to text. Removing special characters is crucial for text processing. Spelling corrections can improve the overall quality of the text. Understanding the sentiment and emotion behind text is important for various applications.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc041d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5efa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b37fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c5fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting emoji\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/03/40/91d0c9fe5a0b494c0fdbcacda4d203aea39f8293e69c70129389308ca928/emoji-2.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
      "   ---------------------------------------- 0.0/397.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/397.5 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 81.9/397.5 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 397.5/397.5 kB 6.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a7fe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/636.8 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 92.2/636.8 kB 871.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 636.8/636.8 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a35783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bhagath Babu S\n",
      "[nltk_data]     L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aebe67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7085d7",
   "metadata": {},
   "source": [
    "# 1. Word Tokenization:\n",
    "Task: Break the text into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bd99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.', 'Identifying', 'stopwords', 'helps', 'in', 'focusing', 'on', 'meaningful', 'words', '.', 'Dependency', 'parsing', 'and', 'POS', 'tagging', 'provide', 'insights', 'into', 'the', 'structure', 'of', 'sentences', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'persons', ',', 'organizations', ',', 'and', 'locations', '.', 'Chunking', 'groups', 'words', 'into', 'meaningful', 'chunks', '.', 'HTML', 'tags', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'removed', 'for', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emotion', 'to', 'text', '.', 'Removing', 'special', 'characters', 'is', 'crucial', 'for', 'text', 'processing', '.', 'Spelling', 'corrections', 'can', 'improve', 'the', 'overall', 'quality', 'of', 'the', 'text', '.', 'Understanding', 'the', 'sentiment', 'and', 'emotion', 'behind', 'text', 'is', 'important', 'for', 'various', 'applications', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7569c",
   "metadata": {},
   "source": [
    "# 2. Stemming:\n",
    "Task: Reduce words to their root/base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697a9b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'fascin', 'field', '.', 'it', 'involv', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'languag', '.', 'token', ',', 'stem', ',', 'and', 'lemmat', 'are', 'essenti', 'step', 'in', 'nlp', '.', 'identifi', 'stopword', 'help', 'in', 'focus', 'on', 'meaning', 'word', '.', 'depend', 'pars', 'and', 'po', 'tag', 'provid', 'insight', 'into', 'the', 'structur', 'of', 'sentenc', '.', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'identifi', 'entiti', 'like', 'person', ',', 'organ', ',', 'and', 'locat', '.', 'chunk', 'group', 'word', 'into', 'meaning', 'chunk', '.', 'html', 'tag', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'remov', 'for', 'plain', 'text', '.', 'emoji', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emot', 'to', 'text', '.', 'remov', 'special', 'charact', 'is', 'crucial', 'for', 'text', 'process', '.', 'spell', 'correct', 'can', 'improv', 'the', 'overal', 'qualiti', 'of', 'the', 'text', '.', 'understand', 'the', 'sentiment', 'and', 'emot', 'behind', 'text', 'is', 'import', 'for', 'variou', 'applic', '.']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in tokens]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224d709",
   "metadata": {},
   "source": [
    "# 3.Stemming: \n",
    "Reduces words to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2fb042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'fascin', 'field', '.', 'it', 'involv', 'the', 'interact', 'between', 'comput', 'and', 'human', 'use', 'natur', 'languag', '.', 'token', ',', 'stem', ',', 'and', 'lemmat', 'are', 'essenti', 'step', 'in', 'nlp', '.', 'identifi', 'stopword', 'help', 'in', 'focus', 'on', 'meaning', 'word', '.', 'depend', 'pars', 'and', 'po', 'tag', 'provid', 'insight', 'into', 'the', 'structur', 'of', 'sentenc', '.', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'identifi', 'entiti', 'like', 'person', ',', 'organ', ',', 'and', 'locat', '.', 'chunk', 'group', 'word', 'into', 'meaning', 'chunk', '.', 'html', 'tag', ',', 'such', 'as', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'remov', 'for', 'plain', 'text', '.', 'emoji', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emot', 'to', 'text', '.', 'remov', 'special', 'charact', 'is', 'crucial', 'for', 'text', 'process', '.', 'spell', 'correct', 'can', 'improv', 'the', 'overal', 'qualiti', 'of', 'the', 'text', '.', 'understand', 'the', 'sentiment', 'and', 'emot', 'behind', 'text', 'is', 'import', 'for', 'variou', 'applic', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "Stemmed_words=[ps.stem(word) for word in tokens]\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245f5288",
   "metadata": {},
   "source": [
    "# 4. Lemmatization\n",
    "Reduce words to their base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c2b461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bhagath Babu S\n",
      "[nltk_data]     L\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'step', 'in', 'NLP', '.', 'Identifying', 'stopwords', 'help', 'in', 'focusing', 'on', 'meaningful', 'word', '.', 'Dependency', 'parsing', 'and', 'POS', 'tagging', 'provide', 'insight', 'into', 'the', 'structure', 'of', 'sentence', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entity', 'like', 'person', ',', 'organization', ',', 'and', 'location', '.', 'Chunking', 'group', 'word', 'into', 'meaningful', 'chunk', '.', 'HTML', 'tag', ',', 'such', 'a', '<', 'b', '>', 'and', '<', 'i', '>', ',', 'can', 'be', 'removed', 'for', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'a', 'layer', 'of', 'emotion', 'to', 'text', '.', 'Removing', 'special', 'character', 'is', 'crucial', 'for', 'text', 'processing', '.', 'Spelling', 'correction', 'can', 'improve', 'the', 'overall', 'quality', 'of', 'the', 'text', '.', 'Understanding', 'the', 'sentiment', 'and', 'emotion', 'behind', 'text', 'is', 'important', 'for', 'various', 'application', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatized_words=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(\"Lemmatized words:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10b8e7",
   "metadata": {},
   "source": [
    "# 5. Identifying Stopword\n",
    "are common words that are often removed from text for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87586082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (without stopwords): ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', '.', 'involves', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'lemmatization', 'essential', 'steps', 'NLP', '.', 'Identifying', 'stopwords', 'helps', 'focusing', 'meaningful', 'words', '.', 'Dependency', 'parsing', 'POS', 'tagging', 'provide', 'insights', 'structure', 'sentences', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'identifies', 'entities', 'like', 'persons', ',', 'organizations', ',', 'locations', '.', 'Chunking', 'groups', 'words', 'meaningful', 'chunks', '.', 'HTML', 'tags', ',', '<', 'b', '>', '<', '>', ',', 'removed', 'plain', 'text', '.', 'Emojis', 'like', 'ðŸ˜ƒ', 'add', 'layer', 'emotion', 'text', '.', 'Removing', 'special', 'characters', 'crucial', 'text', 'processing', '.', 'Spelling', 'corrections', 'improve', 'overall', 'quality', 'text', '.', 'Understanding', 'sentiment', 'emotion', 'behind', 'text', 'important', 'various', 'applications', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bhagath Babu S\n",
      "[nltk_data]     L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens (without stopwords):\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc113554",
   "metadata": {},
   "source": [
    "# 6.  Dependency Parsing\n",
    "Dependency parsing analyzes the grammatical structure of sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5840f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1714370311.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[42], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install --upgrade spacy\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "pip install --upgrade spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a488389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32f17efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDependency parsing is an interesting NLP task.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dependency parsing is an interesting NLP task.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80f590c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1136736537.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[56], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install spacy\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm --direct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a37cd50",
   "metadata": {},
   "source": [
    "# 7. Part-of-Speech (POS) Tagging:\n",
    "POS tagging labels each word with its grammatical category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01061a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP. Identifying stopwords helps in focusing on meaningful words. Dependency parsing and POS tagging provide insights into the structure of sentences. Named Entity Recognition (NER) identifies entities like persons, organizations, and locations. Chunking groups words into meaningful chunks. HTML tags, such as <b> and <i>, can be removed for plain text. Emojis like ðŸ˜ƒ add a layer of emotion to text. Removing special characters is crucial for text processing. Spelling corrections can improve the overall quality of the text. Understanding the sentiment and emotion behind text is important for various applications.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "219a9a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bhagath Babu S L\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64bf88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
    "token_list = word_tokenize(text)\n",
    "print(\"Word Tokens:\", token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4d3a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'Tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'are', 'essential', 'steps', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
    "token_list = word_tokenize(text)\n",
    "print(\"Word Tokens:\", token_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceceec2f",
   "metadata": {},
   "source": [
    "# 8. Named Entity Recognition (NER):\n",
    "     NER identifies entities like persons, organizations, and locations in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e536873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.7 MB 653.6 kB/s eta 0:00:03\n",
      "     --------- ------------------------------ 0.4/1.7 MB 4.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.2/1.7 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (11.0.0)\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (3.9.0)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\lib\\site-packages (from nlp) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->nlp) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->nlp) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->nlp) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->nlp) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->nlp) (1.16.0)\n",
      "Installing collected packages: nlp\n",
      "Successfully installed nlp-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96f09873",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[0;32m      2\u001b[0m entities \u001b[38;5;241m=\u001b[39m [(ent\u001b[38;5;241m.\u001b[39mtext, ent\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed Entities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, entities)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d90c68d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the spaCy English model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Your text data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your text data\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596aee0",
   "metadata": {},
   "source": [
    "# 9.  Chunking:\n",
    "    Chunking groups words into meaningful chunks based on grammatical patterns. This example defines a simple grammar for noun phrases (NP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48f028f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m grammar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNP: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m<DT>?<JJ>*<NN>}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m chunk_parser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(grammar)\n\u001b[1;32m----> 3\u001b[0m tree \u001b[38;5;241m=\u001b[39m chunk_parser\u001b[38;5;241m.\u001b[39mparse(pos_tags)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunks:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tree)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_tags' is not defined"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "print(\"Chunks:\", tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb9739",
   "metadata": {},
   "source": [
    "# 10.HTML Tags::\n",
    "    BeautifulSoup is used to parse HTML tags and extract plain text.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0ba3e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain Text (without HTML tags): Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_text = text\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "plain_text = soup.get_text()\n",
    "print(\"Plain Text (without HTML tags):\", plain_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff91c3",
   "metadata": {},
   "source": [
    "# 11.Emoji:\n",
    "    This step extracts emojis from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "61a87d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\bhagath babu s l\\appdata\\roaming\\python\\python311\\site-packages (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8dd533de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 'P', 'y', 't', 'h', 'o', 'n', '!', ' ']\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "text_with_emoji = \"I love Python! ðŸ˜ƒðŸ\"\n",
    "\n",
    "emoji_list = [c for c in text_with_emoji if c in emoji.demojize(text_with_emoji)]\n",
    "print(\"Emojis:\", emoji_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf64d5",
   "metadata": {},
   "source": [
    "# 12. Remove Special Characters:\n",
    "   Explanation: Regular expressions are used to remove special characters from the text.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27c10408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt=text\n",
    "clean_txt=re.sub(r'[^a-zA-Z0-9\\s]', '',txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c9715",
   "metadata": {},
   "source": [
    "\n",
    "# 13. Spelling Corrections:\n",
    "    Explanation: TextBlob is used for automatic spelling corrections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "64034fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, steaming, and lemmatization are essential steps in NLP.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "txt=text\n",
    "blob=TextBlob(text)\n",
    "corrected_text=str(blob.correct())\n",
    "print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa762f2",
   "metadata": {},
   "source": [
    "# 14. Emotion Analysis:\n",
    "    TextBlob is used for sentiment analysis and emotion assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6456aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "Emotion: Sentiment(polarity=1.0, subjectivity=1.0, assessments=[(['happy', '!'], 1.0, 1.0, None)])\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "txt=text\n",
    "# text = \"I am so happy today!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Accessing sentiment and emotion\n",
    "sentiment = blob.sentiment\n",
    "emotion = blob.sentiment_assessments\n",
    "\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Emotion:\", emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b45dfea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "Emotion: Sentiment(polarity=1.0, subjectivity=1.0, assessments=[(['happy', '!'], 1.0, 1.0, None)])\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "txt=text\n",
    "# text = \"I am so happy today!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Accessing sentiment and emotion\n",
    "sentiment = blob.sentiment\n",
    "emotion = blob.sentiment_assessments\n",
    "\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Emotion:\", emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd51786",
   "metadata": {},
   "source": [
    "# 15. Remove Numbers:\n",
    "Explanation: This step removes numeric digits from the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7fe37291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Numbers: Processing text with numbers like  and .\n"
     ]
    }
   ],
   "source": [
    "text_with_numbers = \"Processing text with numbers like 123 and 456.\"\n",
    "cleaned_text = re.sub(r'\\d+', '', text_with_numbers)\n",
    "print(\"Text without Numbers:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e7c44",
   "metadata": {},
   "source": [
    "# 16. Lowercasing:\n",
    "Explanation: Convert all text to lowercase. This helps in standardizing text for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e733983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Text: i am so happy today!\n"
     ]
    }
   ],
   "source": [
    "lowercased_text = text.lower()\n",
    "print(\"Lowercased Text:\", lowercased_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dd3c4",
   "metadata": {},
   "source": [
    "# 17. Remove Punctuation:\n",
    "Explanation: Remove punctuation marks from the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "caf7b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation: Text with some punctuations\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text_with_punctuation = \"Text with! some@ punctuations#.\"\n",
    "cleaned_text = text_with_punctuation.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(\"Text without Punctuation:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0555b",
   "metadata": {},
   "source": [
    "# 18. Tokenization (Sentence-level):\n",
    " Tokenize the text into sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13e4a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Natural Language Processing (NLP) is a fascinating field.', 'It involves the interaction between computers and humans using natural language.', 'Tokenization, stemming, and lemmatization are essential steps in NLP.', 'Identifying stopwords helps in focusing on meaningful words.', 'Dependency parsing and POS tagging provide insights into the structure of sentences.', 'Named Entity Recognition (NER) identifies entities like persons, organizations, and locations.', 'Chunking groups words into meaningful chunks.', 'HTML tags, such as <b> and <i>, can be removed for plain text.', 'Emojis like ðŸ˜ƒ add a layer of emotion to text.', 'Removing special characters is crucial for text processing.', 'Spelling corrections can improve the overall quality of the text.', 'Understanding the sentiment and emotion behind text is important for various applications.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field. It involves the interaction between computers and humans using natural language. Tokenization, stemming, and lemmatization are essential steps in NLP. Identifying stopwords helps in focusing on meaningful words. Dependency parsing and POS tagging provide insights into the structure of sentences. Named Entity Recognition (NER) identifies entities like persons, organizations, and locations. Chunking groups words into meaningful chunks. HTML tags, such as <b> and <i>, can be removed for plain text. Emojis like ðŸ˜ƒ add a layer of emotion to text. Removing special characters is crucial for text processing. Spelling corrections can improve the overall quality of the text. Understanding the sentiment and emotion behind text is important for various applications.\"\n",
    "\n",
    "sentences=sent_tokenize(text)\n",
    "print(\"Sentences:\",sentences )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf910c9",
   "metadata": {},
   "source": [
    "# 19. Bag of Words (BoW) Representation:\n",
    "Convert text into a matrix of token counts using the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2b35c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words(BOW) Representation:\n",
      "[[1 1 1 0 1 1]\n",
      " [2 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\"]\n",
    "vect=CountVectorizer()\n",
    "x=vect.fit_transform(corpus)\n",
    "print(\"Bag of words(BOW) Representation:\")\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4324bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "20. TF-IDF (Term Frequency-Inverse Document Frequency):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce159f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d486811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f3853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8306784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
